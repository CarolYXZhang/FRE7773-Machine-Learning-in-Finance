{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data as DATA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stock Price Data\n",
    "#### Here we first load all the stock price in our portfolio. This is the raw data of our input and it will later be used in calculation the performance of model and benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JPM</th>\n",
       "      <th>T</th>\n",
       "      <th>PG</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>XOM</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>NEE</th>\n",
       "      <th>BA</th>\n",
       "      <th>AMT</th>\n",
       "      <th>LIN</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-12-31</th>\n",
       "      <td>31.53</td>\n",
       "      <td>28.50</td>\n",
       "      <td>61.82</td>\n",
       "      <td>51.28</td>\n",
       "      <td>79.83</td>\n",
       "      <td>59.83</td>\n",
       "      <td>50.33</td>\n",
       "      <td>42.67</td>\n",
       "      <td>29.32</td>\n",
       "      <td>59.36</td>\n",
       "      <td>12.1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-02</th>\n",
       "      <td>31.35</td>\n",
       "      <td>29.42</td>\n",
       "      <td>62.80</td>\n",
       "      <td>54.36</td>\n",
       "      <td>81.64</td>\n",
       "      <td>60.65</td>\n",
       "      <td>51.66</td>\n",
       "      <td>45.25</td>\n",
       "      <td>30.19</td>\n",
       "      <td>62.43</td>\n",
       "      <td>12.9643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-05</th>\n",
       "      <td>29.25</td>\n",
       "      <td>28.43</td>\n",
       "      <td>62.35</td>\n",
       "      <td>54.06</td>\n",
       "      <td>81.63</td>\n",
       "      <td>60.05</td>\n",
       "      <td>51.94</td>\n",
       "      <td>46.17</td>\n",
       "      <td>30.07</td>\n",
       "      <td>63.87</td>\n",
       "      <td>13.5114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>29.88</td>\n",
       "      <td>28.30</td>\n",
       "      <td>62.17</td>\n",
       "      <td>57.36</td>\n",
       "      <td>80.30</td>\n",
       "      <td>59.69</td>\n",
       "      <td>51.21</td>\n",
       "      <td>46.31</td>\n",
       "      <td>30.23</td>\n",
       "      <td>66.41</td>\n",
       "      <td>13.2886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>28.09</td>\n",
       "      <td>27.21</td>\n",
       "      <td>61.08</td>\n",
       "      <td>56.20</td>\n",
       "      <td>78.25</td>\n",
       "      <td>59.13</td>\n",
       "      <td>50.65</td>\n",
       "      <td>44.76</td>\n",
       "      <td>29.06</td>\n",
       "      <td>62.58</td>\n",
       "      <td>13.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              JPM      T     PG   AMZN    XOM    JNJ    NEE     BA    AMT  \\\n",
       "date                                                                        \n",
       "2008-12-31  31.53  28.50  61.82  51.28  79.83  59.83  50.33  42.67  29.32   \n",
       "2009-01-02  31.35  29.42  62.80  54.36  81.64  60.65  51.66  45.25  30.19   \n",
       "2009-01-05  29.25  28.43  62.35  54.06  81.63  60.05  51.94  46.17  30.07   \n",
       "2009-01-06  29.88  28.30  62.17  57.36  80.30  59.69  51.21  46.31  30.23   \n",
       "2009-01-07  28.09  27.21  61.08  56.20  78.25  59.13  50.65  44.76  29.06   \n",
       "\n",
       "              LIN     AAPL  \n",
       "date                        \n",
       "2008-12-31  59.36  12.1929  \n",
       "2009-01-02  62.43  12.9643  \n",
       "2009-01-05  63.87  13.5114  \n",
       "2009-01-06  66.41  13.2886  \n",
       "2009-01-07  62.58  13.0014  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst={'AAPL','AMT','AMZN','BA','JNJ','JPM','LIN','NEE','PG','T','XOM'}\n",
    "df_stock=pd.DataFrame(data=None)\n",
    "for stock in lst:\n",
    "    data=pd.read_excel('./FRE7773Data/'+stock+'.xlsx',header=None,index_col=None,usecols=[0,1])[8:]\n",
    "    data.rename(columns={0:'date',1:stock},inplace=True)\n",
    "    data['date']=pd.to_datetime(data['date'])\n",
    "    data.set_index('date',inplace=True)\n",
    "    df_stock=pd.concat([df_stock,data], axis=1)\n",
    "    # df=df.sort_values(by = 'date')\n",
    "df_stock=df_stock.sort_index()\n",
    "df_stock=df_stock.astype(float)\n",
    "df_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess Input Data\n",
    "#### Input Data is preprocessed in another code file which gets the covariance and variance from raw stock prices of the assets. According to the result of rolling backtest and grid research, the time window of covariance and variance (TW1)  is set to 60 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2686, 77)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TW1=60\n",
    "# df=pd.read_csv('../tw='+str(TW1),index_col = 0)#Change path when this doesn't match yours\n",
    "df=pd.read_csv('./FRE7773Data/tw='+str(TW1)+'.csv',index_col = 0)\n",
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Matrix is (2686,77), which means there are 77 features and 2686 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Output Data Calculated with HRP\n",
    "#### Output data is calculated using the returns in X future days using HRP algorithm. The calculation can be found in another code file. Here we load the result of calculation as our output. X is also decided by rolling backtest and grid research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2736, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TW2=10\n",
    "# y =pd.read_csv('../Y_HRP_v2/Y_HRP_tw='+str(TW2)+'.csv',index_col = 0)\n",
    "y =pd.read_csv('./Y_HRP_v2/Y_HRP_tw='+str(TW2)+'.csv',index_col = 0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Treasury Yield 10 Years (^TNX) \n",
    "#### Take this as the proxy of risk free rate to calculate Sharpe Ratio later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = DATA.DataReader('^TNX', 'yahoo', '12/31/2008','11/30/2019')\n",
    "rf_data = rf_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Input Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Split dataset\n",
    "#### Here we still use rolling time window to access the performance of our model. The length of time window is set to 3 years and the step is set to 1 year. As validation data are used to tune hyperparameters of the model, it's not needed here. Therefore, validation part is included in training data to make the best use of data information. So the split ratio is 9:1 for training to testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(SW_i,SW_step,SW_len,df,y):\n",
    "    # SW_i : index of time window\n",
    "    # SW_step: the step between the adjacent time windows\n",
    "    # SW_len: the length of time window\n",
    "    # df,y: input and outpu\n",
    "    ind_train_s=SW_i*SW_step\n",
    "    ind_test_s=int(SW_i*SW_step+SW_len*0.9)\n",
    "    ind_test_end=SW_i*SW_step+SW_len\n",
    "    train_data = df.iloc[ind_train_s:ind_test_s,:] \n",
    "    test_data = df.iloc[ind_test_s:ind_test_end,:]\n",
    "    y_train = y.iloc[ind_train_s:ind_test_s,:]\n",
    "    y_test = y.iloc[ind_test_s:ind_test_end,:]\n",
    "    train_data = train_data.values.reshape(-1,77)\n",
    "    test_data = test_data.values.reshape(-1,77)\n",
    "    return (train_data,test_data,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalize data\n",
    "#### The input data is acquired from the stock price without normalization while stock prices vary from each other a lot. Here we normalize it using only the training data. This means the test data is normalized with the information of training data. As the time length of rolling window is set to 3 years, there is no need to cut the normalization to several periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(train_data,test_data):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_data)\n",
    "    train_data= scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    return(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Shape data for LSTM \n",
    "#### LSTM is trained with a 3-dimension input while our input is 2-dimension. Here we need to shape data according to the number of time steps to be considered in one training. Here the number is set to 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, r, TIME_STEPS):\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "#     y = np.zeros((dim_0, r.shape[1]))\n",
    "    \n",
    "    for i in range(dim_0):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "#         y[i] = r[i:i+1]\n",
    "    y = pd.DataFrame(data=r.iloc[:dim_0,:])\n",
    "    print(\"length of time-series input/output:\",x.shape,y.shape)\n",
    "    return x, y\n",
    "\n",
    "def ShapeLSTM(train_data,y_train,test_data,y_test,TS):\n",
    "    X_train, Y_train = build_timeseries(train_data, y_train, TS)\n",
    "    #X_train, Y_train = trim_dataset(X_train, BATCH_SIZE), trim_dataset(Y_train, BATCH_SIZE)\n",
    "    X_test, Y_test = build_timeseries(test_data, y_test, TS)\n",
    "    #X_test, Y_test = trim_dataset(X_test, BATCH_SIZE), trim_dataset(Y_test, BATCH_SIZE)\n",
    "    return(X_train,Y_train,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train Model\n",
    "#### According to previous result, LSTM performs better than ANN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildNTrainModel(TS,X_train,Y_train):\n",
    "    model = Sequential()\n",
    "    # model.add(LSTM(32, batch_input_shape=(10, x_train.shape[1], x_train.shape[2]), stateful=True))\n",
    "    model.add(LSTM(32, input_shape=(TS, X_train.shape[2]), dropout=0.2, activation='relu'))\n",
    "    #model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(11, activation='linear'))\n",
    "    model.summary()\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    #Train\n",
    "    callback = EarlyStopping(monitor='loss', patience=3, mode='auto', min_delta=0.0001)\n",
    "    history = model.fit(X_train, Y_train, epochs=20, batch_size=20,callbacks=[callback])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalCumReturn(data, tw):\n",
    "    data = data + 1\n",
    "    Cum = data.to_frame(name=\"Cum_return\")\n",
    "    i = np.linspace(0, tw*(len(data)//tw), num=1+len(data)//tw, endpoint=False)\n",
    "    \n",
    "    Cum = Cum.iloc[i]\n",
    "    Cum = Cum.cumprod()\n",
    "    return Cum\n",
    "\n",
    "def Sample(data,tw):\n",
    "    data = data.to_frame(name=\"return_tw\")\n",
    "    i = np.linspace(0, tw*(len(data)//tw), num=1+len(data)//tw, endpoint=False)\n",
    "    rebalance_r = data.iloc[i]\n",
    "    return rebalance_r\n",
    "\n",
    "def CalMetric(data,tw):\n",
    "    cum=CalCumReturn(data,tw)\n",
    "    df=float(1.0/(len(cum)*tw)*252)\n",
    "    Ann_R=(cum.iloc[-1].values-1)*df\n",
    "    Vol=Sample(data,tw).std().values\n",
    "    Ann_Vol=Vol*((252/tw)**0.5)\n",
    "    return(Ann_R,Ann_Vol)\n",
    "\n",
    "def Sharpe_ratio(r, tw, rf_data, days=252):\n",
    "#     Calcualte annual annual Sharpe ratio\n",
    "#     INPUT: part of return_hrp(rebalance_r), not CumHRP\n",
    "    annual_r, annual_vol = CalMetric(r,tw)[0], CalMetric(r,tw)[1]\n",
    "    \n",
    "    rf = rf_data.loc[r.index[0]:r.index[-1], ['Close']]\n",
    "    rf = rf.mean()*0.01\n",
    "    sharpe_ratio = float((annual_r - rf) / annual_vol)\n",
    "    return sharpe_ratio\n",
    "\n",
    "def CalReturn(data, tw):\n",
    "# #     Time interval is tw, eg: pct_change of the 21st and 1st\n",
    "#     r = data.pct_change(tw)[tw:]\n",
    "#     r.index = data.index[:-tw]\n",
    "# #     r.columns = \"R_\"+r.columns\n",
    "#     Time interval is tw, eg: pct_change of the 20th and 1st\n",
    "    r = data.pct_change(tw-1)[tw-1:]\n",
    "    r.index = data.index[:-(tw-1)]\n",
    "#     r.columns = \"R_\"+r.columns\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictNCal(model,X_test,Y_test,tw):\n",
    "    y_pred=model.predict(X_test,batch_size=20)\n",
    "    y_pred_df=pd.DataFrame(data=y_pred,index=Y_test.index,columns=Y_test.columns)\n",
    "    y_normalized=y_pred_df.div(y_pred_df.sum(axis=1), axis=0)\n",
    "    #\n",
    "    df_return=CalReturn(df_stock,tw)\n",
    "    df_return=df_return.astype(float)\n",
    "    df_return=df_return.reindex(y_normalized.columns, axis=1)\n",
    "    df_return_period = df_return[df_return.index.isin(y_normalized.index)]\n",
    "    #\n",
    "    meanweighted=df_return_period.mean(axis=1)\n",
    "    model_result=pd.DataFrame(df_return_period.values*y_normalized.values,columns=y_normalized.columns,index=meanweighted.index).sum(axis=1)\n",
    "    hrp_result=pd.DataFrame(df_return_period.values*Y_test.values,columns=Y_test.columns,index=meanweighted.index).sum(axis=1)\n",
    "    #\n",
    "    Result=np.zeros((3,3))\n",
    "    Result[0]=[CalMetric(meanweighted,tw)[0], CalMetric(meanweighted,tw)[1], Sharpe_ratio(meanweighted, tw, rf_data, days=252)]\n",
    "    Result[1]=[CalMetric(model_result,tw)[0], CalMetric(model_result,tw)[1], Sharpe_ratio(model_result, tw, rf_data, days=252)]\n",
    "    Result[2]=[CalMetric(hrp_result,tw)[0], CalMetric(hrp_result,tw)[1], Sharpe_ratio(hrp_result, tw, rf_data, days=252)]\n",
    "    Result_df=pd.DataFrame(Result,columns=['AnnualReturn','Volatility','SharpeRatio'],index=['Mean','Model','HRP'])\n",
    "    Result_df['AnnualReturn']=pd.Series([\"{0:.2f}%\".format(val * 100) for val in Result_df['AnnualReturn']], index = Result_df.index)\n",
    "    return Result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Rolling Time Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of time-series input/output: (620, 60, 77) (620, 11)\n",
      "length of time-series input/output: (16, 60, 77) (16, 11)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                14080     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 14,443\n",
      "Trainable params: 14,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 0.4929\n",
      "Epoch 2/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0315\n",
      "Epoch 3/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0221\n",
      "Epoch 4/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 0.0179\n",
      "Epoch 5/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0155\n",
      "Epoch 6/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0141\n",
      "Epoch 7/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0129\n",
      "Epoch 8/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0117\n",
      "Epoch 9/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0117\n",
      "Epoch 10/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0101\n",
      "Epoch 11/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0095\n",
      "Epoch 12/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0086\n",
      "Epoch 13/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0080\n",
      "Epoch 14/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0074\n",
      "Epoch 15/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0069\n",
      "Epoch 16/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0068\n",
      "Epoch 17/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0066\n",
      "Epoch 18/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0066\n",
      "Epoch 19/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0065\n",
      "Epoch 20/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0065\n",
      "length of time-series input/output: (620, 60, 77) (620, 11)\n",
      "length of time-series input/output: (16, 60, 77) (16, 11)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 32)                14080     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 14,443\n",
      "Trainable params: 14,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 1.0693\n",
      "Epoch 2/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0468\n",
      "Epoch 3/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0246\n",
      "Epoch 4/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0181\n",
      "Epoch 5/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0145\n",
      "Epoch 6/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0124\n",
      "Epoch 7/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0115\n",
      "Epoch 8/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0102\n",
      "Epoch 9/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0094\n",
      "Epoch 10/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0083\n",
      "Epoch 11/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0078\n",
      "Epoch 12/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0068\n",
      "Epoch 13/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0068\n",
      "Epoch 14/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0065\n",
      "Epoch 15/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0065\n",
      "Epoch 16/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0062\n",
      "Epoch 17/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0060\n",
      "Epoch 18/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0060\n",
      "Epoch 19/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0059\n",
      "Epoch 20/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0058\n",
      "length of time-series input/output: (620, 60, 77) (620, 11)\n",
      "length of time-series input/output: (16, 60, 77) (16, 11)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 32)                14080     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 14,443\n",
      "Trainable params: 14,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 0.0504\n",
      "Epoch 2/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0136\n",
      "Epoch 3/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0105\n",
      "Epoch 4/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0093\n",
      "Epoch 5/20\n",
      "620/620 [==============================] - 8s 14ms/step - loss: 0.0081\n",
      "Epoch 6/20\n",
      "620/620 [==============================] - 8s 13ms/step - loss: 0.0077\n",
      "Epoch 7/20\n",
      "620/620 [==============================] - 10s 16ms/step - loss: 0.0072\n",
      "Epoch 8/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0070\n",
      "Epoch 9/20\n",
      "620/620 [==============================] - 12s 19ms/step - loss: 0.0068\n",
      "Epoch 10/20\n",
      "620/620 [==============================] - 12s 20ms/step - loss: 0.0067\n",
      "Epoch 11/20\n",
      "620/620 [==============================] - 9s 15ms/step - loss: 0.0065\n",
      "Epoch 12/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0064\n",
      "Epoch 13/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 0.0063\n",
      "Epoch 14/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0063\n",
      "Epoch 15/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0062\n",
      "Epoch 16/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0061\n",
      "Epoch 17/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0060\n",
      "Epoch 18/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0059\n",
      "Epoch 19/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0059\n",
      "Epoch 20/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0058\n",
      "length of time-series input/output: (620, 60, 77) (620, 11)\n",
      "length of time-series input/output: (16, 60, 77) (16, 11)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 32)                14080     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 14,443\n",
      "Trainable params: 14,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "620/620 [==============================] - 8s 12ms/step - loss: 0.0397\n",
      "Epoch 2/20\n",
      "620/620 [==============================] - 7s 12ms/step - loss: 0.0116\n",
      "Epoch 3/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 0.0096\n",
      "Epoch 4/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 0.0088\n",
      "Epoch 5/20\n",
      "620/620 [==============================] - 8s 14ms/step - loss: 0.0081\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0078\n",
      "Epoch 7/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0076\n",
      "Epoch 8/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 0.0074\n",
      "Epoch 9/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0073\n",
      "Epoch 10/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0073\n",
      "Epoch 11/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0071\n",
      "Epoch 12/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0071\n",
      "Epoch 13/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0070\n",
      "Epoch 14/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0069\n",
      "Epoch 15/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0069\n",
      "Epoch 16/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0068\n",
      "Epoch 17/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0067\n",
      "Epoch 18/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0067\n",
      "Epoch 19/20\n",
      "620/620 [==============================] - 7s 11ms/step - loss: 0.0066\n",
      "Epoch 20/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0066\n",
      "length of time-series input/output: (620, 60, 77) (620, 11)\n",
      "length of time-series input/output: (16, 60, 77) (16, 11)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 32)                14080     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 14,443\n",
      "Trainable params: 14,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "620/620 [==============================] - 9s 15ms/step - loss: 0.0254\n",
      "Epoch 2/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0097\n",
      "Epoch 3/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0082\n",
      "Epoch 4/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0076\n",
      "Epoch 5/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0072\n",
      "Epoch 6/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0070\n",
      "Epoch 7/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0068\n",
      "Epoch 8/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0066\n",
      "Epoch 9/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0066\n",
      "Epoch 10/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0066\n",
      "Epoch 11/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0064\n",
      "Epoch 12/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0064\n",
      "Epoch 13/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0064\n",
      "Epoch 14/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0063\n",
      "Epoch 15/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0062\n",
      "Epoch 16/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0062\n",
      "Epoch 17/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0062\n",
      "Epoch 18/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0061\n",
      "Epoch 19/20\n",
      "620/620 [==============================] - 8s 13ms/step - loss: 0.0062\n",
      "length of time-series input/output: (620, 60, 77) (620, 11)\n",
      "length of time-series input/output: (16, 60, 77) (16, 11)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 32)                14080     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 14,443\n",
      "Trainable params: 14,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "620/620 [==============================] - 10s 16ms/step - loss: 4056.5961\n",
      "Epoch 2/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.2998\n",
      "Epoch 3/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.2635\n",
      "Epoch 4/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.2247\n",
      "Epoch 5/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 106.5037\n",
      "Epoch 6/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 3.3590\n",
      "Epoch 7/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 48.8358\n",
      "length of time-series input/output: (620, 60, 77) (620, 11)\n",
      "length of time-series input/output: (16, 60, 77) (16, 11)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 32)                14080     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 11)                363       \n",
      "=================================================================\n",
      "Total params: 14,443\n",
      "Trainable params: 14,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "620/620 [==============================] - 10s 16ms/step - loss: 1.2862\n",
      "Epoch 2/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0269\n",
      "Epoch 3/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0205\n",
      "Epoch 4/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0156\n",
      "Epoch 5/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0149\n",
      "Epoch 6/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0130\n",
      "Epoch 7/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0117\n",
      "Epoch 8/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0111\n",
      "Epoch 9/20\n",
      "620/620 [==============================] - 5s 7ms/step - loss: 0.0107\n",
      "Epoch 10/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0102\n",
      "Epoch 11/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0097\n",
      "Epoch 12/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0092\n",
      "Epoch 13/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0101\n",
      "Epoch 14/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0084\n",
      "Epoch 15/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0084\n",
      "Epoch 16/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0082\n",
      "Epoch 17/20\n",
      "620/620 [==============================] - 5s 8ms/step - loss: 0.0080\n",
      "Epoch 18/20\n",
      "620/620 [==============================] - 6s 10ms/step - loss: 0.0079\n",
      "Epoch 19/20\n",
      "620/620 [==============================] - 6s 9ms/step - loss: 0.0075\n",
      "Epoch 20/20\n",
      "620/620 [==============================] - 5s 9ms/step - loss: 0.0075\n"
     ]
    }
   ],
   "source": [
    "SW_Result=pd.DataFrame(None)\n",
    "SW_step=252\n",
    "SW_len=756\n",
    "for SW_i in range(0,7):\n",
    "    TS=60\n",
    "    tw=TW2\n",
    "    train_data,test_data,y_train,y_test = SplitData(SW_i,SW_step,SW_len,df,y)\n",
    "    train_data,test_data = Normalization(train_data,test_data)\n",
    "    X_train,Y_train,X_test,Y_test = ShapeLSTM(train_data,y_train,test_data,y_test,TS)\n",
    "    model = BuildNTrainModel(TS,X_train,Y_train)\n",
    "    Result_df = PredictNCal(model,X_test,Y_test,tw)\n",
    "    SW_Result=pd.concat([SW_Result,Result_df.T.unstack(level=-1)],axis=1)\n",
    "    SW_Result.rename(columns={0:'SW'+str(SW_i)},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SW_Result.to_csv('SW_Result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SW0</th>\n",
       "      <th>SW1</th>\n",
       "      <th>SW2</th>\n",
       "      <th>SW3</th>\n",
       "      <th>SW4</th>\n",
       "      <th>SW5</th>\n",
       "      <th>SW6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Mean</th>\n",
       "      <th>AnnualReturn</th>\n",
       "      <td>-27.25%</td>\n",
       "      <td>-20.50%</td>\n",
       "      <td>-21.31%</td>\n",
       "      <td>-51.81%</td>\n",
       "      <td>16.13%</td>\n",
       "      <td>21.69%</td>\n",
       "      <td>17.48%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volatility</th>\n",
       "      <td>0.0956373</td>\n",
       "      <td>0.035482</td>\n",
       "      <td>0.00903989</td>\n",
       "      <td>0.0790736</td>\n",
       "      <td>0.21227</td>\n",
       "      <td>0.0502168</td>\n",
       "      <td>0.070506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SharpeRatio</th>\n",
       "      <td>-3.04975</td>\n",
       "      <td>-6.26176</td>\n",
       "      <td>-26.5297</td>\n",
       "      <td>-6.8681</td>\n",
       "      <td>0.660701</td>\n",
       "      <td>3.99134</td>\n",
       "      <td>2.15266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Model</th>\n",
       "      <th>AnnualReturn</th>\n",
       "      <td>-6.28%</td>\n",
       "      <td>-2.23%</td>\n",
       "      <td>-36.66%</td>\n",
       "      <td>-63.47%</td>\n",
       "      <td>22.27%</td>\n",
       "      <td>-12.28%</td>\n",
       "      <td>1224.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volatility</th>\n",
       "      <td>0.0709236</td>\n",
       "      <td>0.0214861</td>\n",
       "      <td>0.00300717</td>\n",
       "      <td>0.0830322</td>\n",
       "      <td>0.211738</td>\n",
       "      <td>0.0514242</td>\n",
       "      <td>2.46387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SharpeRatio</th>\n",
       "      <td>-1.15645</td>\n",
       "      <td>-1.83625</td>\n",
       "      <td>-130.768</td>\n",
       "      <td>-7.94498</td>\n",
       "      <td>0.952348</td>\n",
       "      <td>-2.70852</td>\n",
       "      <td>4.96127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">HRP</th>\n",
       "      <th>AnnualReturn</th>\n",
       "      <td>-39.41%</td>\n",
       "      <td>-6.52%</td>\n",
       "      <td>-39.30%</td>\n",
       "      <td>-37.73%</td>\n",
       "      <td>17.76%</td>\n",
       "      <td>6.89%</td>\n",
       "      <td>17.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volatility</th>\n",
       "      <td>0.164357</td>\n",
       "      <td>0.0388425</td>\n",
       "      <td>0.0254195</td>\n",
       "      <td>0.0285272</td>\n",
       "      <td>0.182238</td>\n",
       "      <td>0.0440473</td>\n",
       "      <td>0.0461091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SharpeRatio</th>\n",
       "      <td>-2.5144</td>\n",
       "      <td>-2.12074</td>\n",
       "      <td>-16.5115</td>\n",
       "      <td>-14.1019</td>\n",
       "      <td>0.859037</td>\n",
       "      <td>1.19086</td>\n",
       "      <td>3.34024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          SW0        SW1         SW2        SW3       SW4  \\\n",
       "Mean  AnnualReturn    -27.25%    -20.50%     -21.31%    -51.81%    16.13%   \n",
       "      Volatility    0.0956373   0.035482  0.00903989  0.0790736   0.21227   \n",
       "      SharpeRatio    -3.04975   -6.26176    -26.5297    -6.8681  0.660701   \n",
       "Model AnnualReturn     -6.28%     -2.23%     -36.66%    -63.47%    22.27%   \n",
       "      Volatility    0.0709236  0.0214861  0.00300717  0.0830322  0.211738   \n",
       "      SharpeRatio    -1.15645   -1.83625    -130.768   -7.94498  0.952348   \n",
       "HRP   AnnualReturn    -39.41%     -6.52%     -39.30%    -37.73%    17.76%   \n",
       "      Volatility     0.164357  0.0388425   0.0254195  0.0285272  0.182238   \n",
       "      SharpeRatio     -2.5144   -2.12074    -16.5115   -14.1019  0.859037   \n",
       "\n",
       "                          SW5        SW6  \n",
       "Mean  AnnualReturn     21.69%     17.48%  \n",
       "      Volatility    0.0502168   0.070506  \n",
       "      SharpeRatio     3.99134    2.15266  \n",
       "Model AnnualReturn    -12.28%   1224.69%  \n",
       "      Volatility    0.0514242    2.46387  \n",
       "      SharpeRatio    -2.70852    4.96127  \n",
       "HRP   AnnualReturn      6.89%     17.70%  \n",
       "      Volatility    0.0440473  0.0461091  \n",
       "      SharpeRatio     1.19086    3.34024  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SW_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
